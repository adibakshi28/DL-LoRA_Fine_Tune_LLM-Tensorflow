{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTX60gQVcD2E",
        "outputId": "98a7df2a-170d-45bd-9042-a87fad6c0df5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.40.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.63.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.99)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "pip install datasets tensorflow \"transformers[sentencepiece]\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignore annoying warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "_QslbmQ0eX4j"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training only classifier head - Model : \"distilbert-base-uncased\""
      ],
      "metadata": {
        "id": "IaReTYEqgHze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset and tokenizer\n",
        "dataset = load_dataset(\"financial_phrasebank\", \"sentences_allagree\")\n",
        "checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"sentence\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "train_val_split = tokenized_datasets[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
        "val_test_split = train_val_split['test'].train_test_split(test_size=0.5, seed=42)\n",
        "\n",
        "def to_tf_dataset(split, shuffle=False):\n",
        "    return split.to_tf_dataset(\n",
        "        columns=[\"input_ids\", \"attention_mask\"],\n",
        "        label_cols=[\"label\"],\n",
        "        shuffle=shuffle,\n",
        "        batch_size=8,\n",
        "        collate_fn=None\n",
        "    )\n",
        "\n",
        "tf_train_dataset = to_tf_dataset(train_val_split['train'], shuffle=True)\n",
        "tf_validation_dataset = to_tf_dataset(val_test_split['train'], shuffle=True)\n",
        "tf_test_dataset = to_tf_dataset(val_test_split['test'], shuffle=False)\n",
        "\n",
        "# Define and compile model\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)\n",
        "model.distilbert.trainable = False\n",
        "\n",
        "initial_learning_rate = 5e-5\n",
        "lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "    initial_learning_rate=initial_learning_rate,\n",
        "    decay_steps=10000,\n",
        "    end_learning_rate=0.0,\n",
        "    power=1.0\n",
        ")\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Training\n",
        "model.fit(tf_train_dataset, epochs=3, validation_data=tf_validation_dataset)\n",
        "\n",
        "# Evaluation\n",
        "eval_loss, eval_accuracy = model.evaluate(tf_test_dataset)\n",
        "print(f\"Evaluated Test Loss: {eval_loss}, Evaluated Test Accuracy: {eval_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoxVRAv7eZSV",
        "outputId": "853546f1-7aa2-4895-b91c-c50ac5c77bfc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_119', 'pre_classifier', 'classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_distil_bert_for_sequence_classification_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " distilbert (TFDistilBertMa  multiple                  66362880  \n",
            " inLayer)                                                        \n",
            "                                                                 \n",
            " pre_classifier (Dense)      multiple                  590592    \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  2307      \n",
            "                                                                 \n",
            " dropout_119 (Dropout)       multiple                  0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 66955779 (255.42 MB)\n",
            "Trainable params: 592899 (2.26 MB)\n",
            "Non-trainable params: 66362880 (253.15 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving Model Weights of only classifier head trained model"
      ],
      "metadata": {
        "id": "2kq96k_ugX_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights('classifier_head_weights.h5')"
      ],
      "metadata": {
        "id": "QDFbk71lehEP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keeping the trained Classifier head of the first part: Fine-tune **all** the weights\n",
        "\n",
        "Using conventional approach\n"
      ],
      "metadata": {
        "id": "Qj1J2yebf97-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained model for fine-tuning all layers except the classifier\n",
        "model_minus_classifier = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)\n",
        "\n",
        "# Now load the weights saved back into the model\n",
        "model_minus_classifier.load_weights('classifier_head_weights.h5', by_name=True)\n",
        "\n",
        "# Set only the pre_classifier and classifier layer to non-trainable to preserve its weights\n",
        "model_minus_classifier.pre_classifier.trainable = False\n",
        "model_minus_classifier.classifier.trainable = False\n",
        "\n",
        "initial_learning_rate = 5e-5\n",
        "lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "    initial_learning_rate=initial_learning_rate,\n",
        "    decay_steps=10000,\n",
        "    end_learning_rate=0.0,\n",
        "    power=1.0\n",
        ")\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "model_minus_classifier.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_minus_classifier.summary()\n",
        "\n",
        "history = model_minus_classifier.fit(\n",
        "    tf_train_dataset,\n",
        "    epochs=3,\n",
        "    validation_data=tf_validation_dataset\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model_minus_classifier.evaluate(tf_test_dataset)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_lFv_Jb24nl",
        "outputId": "cd1b1b68-c73c-4465-cfa2-afe0e7c4df74"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_519', 'pre_classifier', 'classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_distil_bert_for_sequence_classification_25\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " distilbert (TFDistilBertMa  multiple                  66362880  \n",
            " inLayer)                                                        \n",
            "                                                                 \n",
            " pre_classifier (Dense)      multiple                  590592    \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  2307      \n",
            "                                                                 \n",
            " dropout_519 (Dropout)       multiple                  0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 66955779 (255.42 MB)\n",
            "Trainable params: 66362880 (253.15 MB)\n",
            "Non-trainable params: 592899 (2.26 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "227/227 [==============================] - 131s 489ms/step - loss: 0.2206 - accuracy: 0.9260 - val_loss: 0.1626 - val_accuracy: 0.9292\n",
            "Epoch 2/3\n",
            "227/227 [==============================] - 110s 484ms/step - loss: 0.0778 - accuracy: 0.9757 - val_loss: 0.1388 - val_accuracy: 0.9513\n",
            "Epoch 3/3\n",
            "227/227 [==============================] - 109s 480ms/step - loss: 0.0281 - accuracy: 0.9906 - val_loss: 0.2725 - val_accuracy: 0.9336\n",
            "29/29 [==============================] - 5s 164ms/step - loss: 0.1957 - accuracy: 0.9559\n",
            "Test Loss: 0.19569317996501923, Test Accuracy: 0.9559471607208252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training:**\n",
        "\n",
        "* Total params: 66955779 (255.42 MB)\n",
        "* Trainable params: 66362880 (253.15 MB)\n",
        "* Non-trainable params: 592899 (2.26 MB)\n",
        "\n",
        "**Result:**\n",
        "\n",
        "* Test Loss: 0.195, Test Accuracy: 95.5%"
      ],
      "metadata": {
        "id": "UzEeVtvV77hp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune **only the weights** of the FFN blocks in every layer using LoRA technique"
      ],
      "metadata": {
        "id": "aRNBGdQ_gqVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFDistilBertForSequenceClassification, DistilBertConfig\n",
        "\n",
        "class LoRALayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, original_layer, rank):\n",
        "        super(LoRALayer, self).__init__()\n",
        "        self.original_layer = original_layer\n",
        "        self.rank = rank\n",
        "        self.input_dim = original_layer.kernel.shape[0]\n",
        "        self.output_dim = original_layer.kernel.shape[1]\n",
        "\n",
        "        # Initialize LoRA parameters\n",
        "        self.A = self.add_weight(shape=(self.input_dim, rank), initializer='glorot_uniform', trainable=True)\n",
        "        self.B = self.add_weight(shape=(rank, self.output_dim), initializer='glorot_uniform', trainable=True)\n",
        "        self.lora_alpha = 1.0\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Compute the original output\n",
        "        original_output = self.original_layer(inputs)\n",
        "\n",
        "        # Compute the low-rank adaptation\n",
        "        lora_update = self.lora_alpha * tf.matmul(inputs, tf.matmul(self.A, self.B))\n",
        "\n",
        "        return original_output + lora_update\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased\"\n",
        "model_lora = TFDistilBertForSequenceClassification.from_pretrained(checkpoint, num_labels=3)\n",
        "\n",
        "# Load the previously saved weights into the model\n",
        "model_lora.load_weights('classifier_head_weights.h5', by_name=True)\n",
        "\n",
        "# Rank for LoRA matrices\n",
        "lora_rank = 8\n",
        "\n",
        "# Replace each lin1 and lin2 layer in each transformer layer with a LoRA-enhanced layer\n",
        "for i, layer in enumerate(model_lora.distilbert.transformer.layer):\n",
        "    original_lin1 = layer.ffn.lin1\n",
        "    original_lin2 = layer.ffn.lin2\n",
        "    layer.ffn.lin1 = LoRALayer(original_lin1, rank=lora_rank)\n",
        "    layer.ffn.lin2 = LoRALayer(original_lin2, rank=lora_rank)\n",
        "\n",
        "    original_lin1.trainable = False\n",
        "    original_lin2.trainable = False\n",
        "\n",
        "# Freeze weights of the classifier head\n",
        "model_lora.pre_classifier.trainable = False\n",
        "model_lora.classifier.trainable = False\n",
        "\n",
        "# Freeze all other parameters except for the LoRA parameters\n",
        "for layer in model_lora.distilbert.transformer.layer:\n",
        "    layer.attention.trainable = False\n",
        "\n",
        "initial_learning_rate = 5e-5\n",
        "lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "    initial_learning_rate=initial_learning_rate,\n",
        "    decay_steps=10000,\n",
        "    end_learning_rate=0.0,\n",
        "    power=1.0\n",
        ")\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "model_lora.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "\n",
        "model_lora.summary()\n",
        "\n",
        "# Train the model\n",
        "model_lora.fit(tf_train_dataset, epochs=3, validation_data=tf_validation_dataset)\n",
        "\n",
        "test_loss, test_accuracy = model_lora.evaluate(tf_test_dataset)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jPI2Nekqn86",
        "outputId": "51aae474-8737-4537-aaca-49bac7170eff"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'dropout_599', 'classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_distil_bert_for_sequence_classification_29\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " distilbert (TFDistilBertMa  multiple                  66731520  \n",
            " inLayer)                                                        \n",
            "                                                                 \n",
            " pre_classifier (Dense)      multiple                  590592    \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  2307      \n",
            "                                                                 \n",
            " dropout_599 (Dropout)       multiple                  0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 67324419 (256.82 MB)\n",
            "Trainable params: 24222720 (92.40 MB)\n",
            "Non-trainable params: 43101699 (164.42 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "227/227 [==============================] - 148s 597ms/step - loss: 0.4028 - accuracy: 0.8161 - val_loss: 0.3618 - val_accuracy: 0.8319\n",
            "Epoch 2/3\n",
            "227/227 [==============================] - 136s 600ms/step - loss: 0.2232 - accuracy: 0.9260 - val_loss: 0.2194 - val_accuracy: 0.9115\n",
            "Epoch 3/3\n",
            "227/227 [==============================] - 137s 603ms/step - loss: 0.1062 - accuracy: 0.9735 - val_loss: 0.1728 - val_accuracy: 0.9248\n",
            "29/29 [==============================] - 6s 219ms/step - loss: 0.1827 - accuracy: 0.9339\n",
            "Test Loss: 0.18266057968139648, Test Accuracy: 0.933920681476593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training:**\n",
        "\n",
        "* Total params: 67324419 (256.82 MB)\n",
        "\n",
        "* Trainable params: 24222720 (92.40 MB)\n",
        "\n",
        "* Non-trainable params: 43101699 (164.42 MB)\n",
        "\n",
        "**Result:**\n",
        "\n",
        "* Test Loss: 0.182, Test Accuracy: 93.3%"
      ],
      "metadata": {
        "id": "Ec_qeFup7pGB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation:\n",
        "LoRA is training just 33% weights as the conventional method but still getting a comparable accuracy to the conventional way of fine tuning"
      ],
      "metadata": {
        "id": "XRMsce__8HOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q) How does the performance of the model fine-tuned with LoRA compare to that of the pre-trained (not fine-tuned) model?"
      ],
      "metadata": {
        "id": "fArBtdsK4RIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting out of sample accuracy for pre-trained distilBERT model (not fine-tuned)"
      ],
      "metadata": {
        "id": "4cUBomLA4WHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained model for fine-tuning all layers except the classifier\n",
        "model_without_pre_training = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)\n",
        "\n",
        "\n",
        "initial_learning_rate = 5e-5\n",
        "lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "    initial_learning_rate=initial_learning_rate,\n",
        "    decay_steps=10000,\n",
        "    end_learning_rate=0.0,\n",
        "    power=1.0\n",
        ")\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "model_without_pre_training.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_without_pre_training.summary()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model_without_pre_training.evaluate(tf_test_dataset)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTQquQJE4oAA",
        "outputId": "fb0528a4-434c-4b26-9924-8125e54660fc"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_579', 'pre_classifier', 'classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_distil_bert_for_sequence_classification_28\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " distilbert (TFDistilBertMa  multiple                  66362880  \n",
            " inLayer)                                                        \n",
            "                                                                 \n",
            " pre_classifier (Dense)      multiple                  590592    \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  2307      \n",
            "                                                                 \n",
            " dropout_579 (Dropout)       multiple                  0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 66955779 (255.42 MB)\n",
            "Trainable params: 66955779 (255.42 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "29/29 [==============================] - 8s 161ms/step - loss: 1.1227 - accuracy: 0.1322\n",
            "Test Loss: 1.122727870941162, Test Accuracy: 0.13215859234333038\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result:\n",
        "\n",
        "Accuracy for pre-trained model (Not fine tuned) : 13.2%\n",
        "\n",
        "Accuracy for LoRA fine tuned model : 93.3%\n",
        "\n",
        "Conclusion: Clearly LoRA fine tuned model is better"
      ],
      "metadata": {
        "id": "-8i51-Fi4o01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q) How does the performance of the model fine-tuned with LoRA compare to that of the model fine-tuned using the conventional approach?"
      ],
      "metadata": {
        "id": "VtLLCagO4WSy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result:\n",
        "\n",
        "Accuracy for fine tuned model using conventional approach (Train whole model except fine tuned head) : 95.5%\n",
        "\n",
        "Accuracy for LoRA fine tuned model (Train whole model with LoRA except fine tuned head) : 93.3%\n",
        "\n",
        "Conclusion: Both the models perform almost equally"
      ],
      "metadata": {
        "id": "e5e46-JT5M5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q) How many LoRA parameters for each FFN block"
      ],
      "metadata": {
        "id": "rnDSIBWkuCBa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Calulate input and output dimensions (d_input and d_output) of the distilbert model"
      ],
      "metadata": {
        "id": "KzZHZFtcuxgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFDistilBertForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased\"\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained(checkpoint, num_labels=3)\n",
        "\n",
        "# Retrieve the input and output dimensions from the first FFN block\n",
        "d_input = model.distilbert.transformer.layer[0].ffn.lin1.kernel.shape[0]\n",
        "d_output = model.distilbert.transformer.layer[0].ffn.lin1.kernel.shape[1]\n",
        "\n",
        "print(f\"Input dimension (d_input): {d_input}\")\n",
        "print(f\"Output dimension (d_output): {d_output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skdqmYfCyv5a",
        "outputId": "5880b044-dbe2-4767-f969-1e6ade7ae96c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_379', 'pre_classifier', 'classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input dimension (d_input): 768\n",
            "Output dimension (d_output): 3072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result:\n",
        "\n",
        "Input dimension (d_input): 768\n",
        "\n",
        "Output dimension (d_output): 3072"
      ],
      "metadata": {
        "id": "DuR9GqUK3Ldi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Calculate total LoRA parameters per FFN block\n",
        "\n",
        "Parameters = (d_input * r) + (r * d_output)\n",
        "\n",
        "where r is the rank used ie 8 in our code"
      ],
      "metadata": {
        "id": "X5iN3cxT0VZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = 8  # Rank used for LoRA\n",
        "\n",
        "# Calculate number of parameters for one FFN block (lin1 and lin2)\n",
        "def calculate_lora_parameters(d_input, d_output, rank):\n",
        "    return (d_input * rank) + (rank * d_output)\n",
        "\n",
        "lora_params_per_layer = calculate_lora_parameters(d_input, d_output, r)\n",
        "\n",
        "lora_params_per_ffn_block = lora_params_per_layer * 2\n",
        "\n",
        "print(f\"Number of LoRA parameters per layer: {lora_params_per_layer}\")\n",
        "print(f\"Total LoRA parameters per FFN block: {lora_params_per_ffn_block}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3Fd2-RVuGJL",
        "outputId": "0b124749-337f-4f68-898f-67edd53c9712"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of LoRA parameters per layer: 30720\n",
            "Total LoRA parameters per FFN block: 61440\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result:\n",
        "\n",
        "Number of LoRA parameters per layer: 30720\n",
        "\n",
        "Total LoRA parameters per FFN block: 61440"
      ],
      "metadata": {
        "id": "IXpLcWsb3SKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q) How many LoRA parameters total (number of FFN blocks times the LoRA parameters per block)"
      ],
      "metadata": {
        "id": "5qMvurFpvkQs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Calculate number of transformer layers in distilbert model"
      ],
      "metadata": {
        "id": "pPXVTY7KwO5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFDistilBertForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased\"\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained(checkpoint, num_labels=3)\n",
        "\n",
        "num_layers = len(model.distilbert.transformer.layer)\n",
        "\n",
        "print(f\"Number of transformer layers: {num_layers}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNcxgVwQyYoK",
        "outputId": "8898e202-b23f-4691-e9a0-1a16a26149f6"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_399', 'pre_classifier', 'classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of transformer layers: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result:\n",
        "\n",
        "Number of transformer layers: 6"
      ],
      "metadata": {
        "id": "A_xUdbNJ3XU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Calculate total added parameters\n",
        "\n",
        "Each transformer layer contains 1 FFN block that consists of two dense layers (lin1 and lin2)\n",
        "\n",
        "*added_total = lora_params_per_ffn_block * num_layers*"
      ],
      "metadata": {
        "id": "diVfSKuQz_pu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "added_total = lora_params_per_ffn_block * num_layers\n",
        "\n",
        "print(f\"Number of LoRA parameters per layer: {lora_params_per_layer}\")\n",
        "print(f\"Total LoRA parameters per FFN block: {lora_params_per_ffn_block}\")\n",
        "print(f\"Total added LoRA parameters across all FFN blocks: {added_total}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPl3uqUevpUL",
        "outputId": "f1145210-2bc9-48f6-b144-b7e8fdf8a4bb"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of LoRA parameters per layer: 30720\n",
            "Total LoRA parameters per FFN block: 61440\n",
            "Total added LoRA parameters across all FFN blocks: 368640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result:\n",
        "\n",
        "Number of LoRA parameters per layer: 30720\n",
        "\n",
        "Total LoRA parameters per FFN block: 61440\n",
        "\n",
        "Total added LoRA parameters across all FFN blocks: 368640"
      ],
      "metadata": {
        "id": "yQJwF4h73aMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q) How many total parameters in your model ?"
      ],
      "metadata": {
        "id": "4x7aMFsLwfhb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Calculate the total number of parameters in the unmodified DistilBERT model"
      ],
      "metadata": {
        "id": "p_3a33M5zare"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFDistilBertForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased\"\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained(checkpoint, num_labels=3)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(np.prod(v.shape) for v in model.trainable_weights)\n",
        "\n",
        "# Calculate the total number of parameters in the unmodified DistilBERT model\n",
        "original_params = count_parameters(model)\n",
        "\n",
        "print(f\"Original DistilBERT parameters: {original_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YPsxJxdxhIF",
        "outputId": "48280c3d-f31a-4495-ecdf-566bf8d9767a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_419', 'pre_classifier', 'classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DistilBERT parameters: 66955779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result:\n",
        "\n",
        "Original DistilBERT parameters: 66955779"
      ],
      "metadata": {
        "id": "GxvmOXvF3g_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Calculate the Total parameters in the model <br>\n",
        "*(total_parameters = original_params + added_total)*\n"
      ],
      "metadata": {
        "id": "k7JfHpCTzhRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Total parameters in the model\n",
        "total_parameters = original_params + added_total\n",
        "\n",
        "print(f\"Original DistilBERT parameters: {original_params}\")\n",
        "print(f\"Added LoRA parameters: {added_total}\")\n",
        "print(f\"Total parameters in the adapted model: {total_parameters}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSYacSujwmlu",
        "outputId": "0211423d-f990-491a-d48b-87a765d76951"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DistilBERT parameters: 66955779\n",
            "Added LoRA parameters: 368640\n",
            "Total parameters in the adapted model: 67324419\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result:\n",
        "\n",
        "Original DistilBERT parameters: 66955779\n",
        "\n",
        "Added LoRA parameters: 368640\n",
        "\n",
        "Total parameters in the adapted model: 67324419"
      ],
      "metadata": {
        "id": "TeqkteGw3li1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# assert(added_total == added_parms_calc)"
      ],
      "metadata": {
        "id": "TflpBsT21Ntv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFDistilBertForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased\"\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "d_input = model.distilbert.transformer.layer[0].ffn.lin1.kernel.shape[0]\n",
        "d_output = model.distilbert.transformer.layer[0].ffn.lin1.kernel.shape[1]\n",
        "num_layers = len(model.distilbert.transformer.layer)\n",
        "r = 8  # Rank used for LoRA\n",
        "\n",
        "def calculate_lora_parameters(d_input, d_output, rank):\n",
        "    return (d_input * rank) + (rank * d_output)\n",
        "\n",
        "lora_params_per_layer = calculate_lora_parameters(d_input, d_output, r)\n",
        "\n",
        "lora_params_per_ffn_block = lora_params_per_layer * 2\n",
        "\n",
        "added_parms_calc = lora_params_per_ffn_block * num_layers\n",
        "\n",
        "added_total = added_parms_calc\n",
        "\n",
        "# Assertion to verify the numbers match\n",
        "assert(added_total == added_parms_calc)\n",
        "\n",
        "print(f\"Calculated trainable parameters added by LoRA: {added_parms_calc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvsErZJx1N41",
        "outputId": "ac04ed76-1f5f-4ace-e4ee-ff0130212f01"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_479', 'pre_classifier', 'classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculated trainable parameters added by LoRA: 368640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result:\n",
        "\n",
        "Calculated trainable parameters added by LoRA: 368640\n",
        "\n",
        "Assertion is true!"
      ],
      "metadata": {
        "id": "vXIlrQAQ3yRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment with the rank used in LoRA\n",
        "\n",
        "Changing rank and checking accuracy"
      ],
      "metadata": {
        "id": "97Hv0ePo-xLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFDistilBertForSequenceClassification, DistilBertConfig\n",
        "\n",
        "class LoRALayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, original_layer, rank):\n",
        "        super(LoRALayer, self).__init__()\n",
        "        self.original_layer = original_layer\n",
        "        self.rank = rank\n",
        "        self.input_dim = original_layer.kernel.shape[0]\n",
        "        self.output_dim = original_layer.kernel.shape[1]\n",
        "\n",
        "        # Initialize LoRA parameters\n",
        "        self.A = self.add_weight(shape=(self.input_dim, rank), initializer='glorot_uniform', trainable=True)\n",
        "        self.B = self.add_weight(shape=(rank, self.output_dim), initializer='glorot_uniform', trainable=True)\n",
        "        self.lora_alpha = 1.0\n",
        "\n",
        "    def call(self, inputs):\n",
        "        original_output = self.original_layer(inputs)\n",
        "        lora_update = self.lora_alpha * tf.matmul(inputs, tf.matmul(self.A, self.B))\n",
        "        return original_output + lora_update\n",
        "\n",
        "# List of ranks to test\n",
        "ranks = [2, 4, 8, 16, 32, 64]\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "for lora_rank in ranks:\n",
        "    print(f\"Training with LoRA rank: {lora_rank}\")\n",
        "    model_lora = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
        "    model_lora.load_weights('classifier_head_weights.h5', by_name=True)\n",
        "\n",
        "    # Apply LoRA layers\n",
        "    for i, layer in enumerate(model_lora.distilbert.transformer.layer):\n",
        "        original_lin1 = layer.ffn.lin1\n",
        "        original_lin2 = layer.ffn.lin2\n",
        "        layer.ffn.lin1 = LoRALayer(original_lin1, rank=lora_rank)\n",
        "        layer.ffn.lin2 = LoRALayer(original_lin2, rank=lora_rank)\n",
        "\n",
        "        original_lin1.trainable = False\n",
        "        original_lin2.trainable = False\n",
        "\n",
        "    # Freeze classifier head\n",
        "    model_lora.pre_classifier.trainable = False\n",
        "    model_lora.classifier.trainable = False\n",
        "\n",
        "    # Freeze all other parameters except for the LoRA parameters\n",
        "    for layer in model_lora.distilbert.transformer.layer:\n",
        "        layer.attention.trainable = False\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "        initial_learning_rate=5e-5,\n",
        "        decay_steps=10000,\n",
        "        end_learning_rate=0.0,\n",
        "        power=1.0\n",
        "    ))\n",
        "    model_lora.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "\n",
        "    model_lora.summary()\n",
        "\n",
        "    # Train the model\n",
        "    model_lora.fit(tf_train_dataset, epochs=3, validation_data=tf_validation_dataset)\n",
        "\n",
        "    # Evaluate the model\n",
        "    test_loss, test_accuracy = model_lora.evaluate(tf_test_dataset)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "    print(f\"Rank: {lora_rank}, Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlGyYrRF-6L9",
        "outputId": "a182d4b9-bff5-4bf9-daef-1b6bde3f3712"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with LoRA rank: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'dropout_679', 'classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_distil_bert_for_sequence_classification_33\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " distilbert (TFDistilBertMa  multiple                  66455040  \n",
            " inLayer)                                                        \n",
            "                                                                 \n",
            " pre_classifier (Dense)      multiple                  590592    \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  2307      \n",
            "                                                                 \n",
            " dropout_679 (Dropout)       multiple                  0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 67047939 (255.77 MB)\n",
            "Trainable params: 23946240 (91.35 MB)\n",
            "Non-trainable params: 43101699 (164.42 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "227/227 [==============================] - 230s 594ms/step - loss: 0.4230 - accuracy: 0.8045 - val_loss: 0.4207 - val_accuracy: 0.7876\n",
            "Epoch 2/3\n",
            "227/227 [==============================] - 136s 599ms/step - loss: 0.3051 - accuracy: 0.8923 - val_loss: 0.3214 - val_accuracy: 0.8407\n",
            "Epoch 3/3\n",
            "227/227 [==============================] - 136s 600ms/step - loss: 0.1842 - accuracy: 0.9481 - val_loss: 0.2307 - val_accuracy: 0.8982\n",
            "29/29 [==============================] - 6s 219ms/step - loss: 0.2493 - accuracy: 0.9163\n",
            "Rank: 2, Test Loss: 0.24926377832889557, Test Accuracy: 0.91629958152771\n",
            "Training with LoRA rank: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'pre_classifier', 'dropout_699']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_distil_bert_for_sequence_classification_34\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " distilbert (TFDistilBertMa  multiple                  66547200  \n",
            " inLayer)                                                        \n",
            "                                                                 \n",
            " pre_classifier (Dense)      multiple                  590592    \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  2307      \n",
            "                                                                 \n",
            " dropout_699 (Dropout)       multiple                  0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 67140099 (256.12 MB)\n",
            "Trainable params: 24038400 (91.70 MB)\n",
            "Non-trainable params: 43101699 (164.42 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "227/227 [==============================] - 153s 611ms/step - loss: 0.4198 - accuracy: 0.8040 - val_loss: 0.3918 - val_accuracy: 0.8142\n",
            "Epoch 2/3\n",
            "227/227 [==============================] - 133s 586ms/step - loss: 0.2772 - accuracy: 0.9050 - val_loss: 0.2669 - val_accuracy: 0.8850\n",
            "Epoch 3/3\n",
            "227/227 [==============================] - 136s 601ms/step - loss: 0.1449 - accuracy: 0.9663 - val_loss: 0.2032 - val_accuracy: 0.9292\n",
            "29/29 [==============================] - 6s 220ms/step - loss: 0.2167 - accuracy: 0.9295\n",
            "Rank: 4, Test Loss: 0.2166893035173416, Test Accuracy: 0.9295154213905334\n",
            "Training with LoRA rank: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_719', 'pre_classifier', 'classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_distil_bert_for_sequence_classification_35\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " distilbert (TFDistilBertMa  multiple                  66731520  \n",
            " inLayer)                                                        \n",
            "                                                                 \n",
            " pre_classifier (Dense)      multiple                  590592    \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  2307      \n",
            "                                                                 \n",
            " dropout_719 (Dropout)       multiple                  0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 67324419 (256.82 MB)\n",
            "Trainable params: 24222720 (92.40 MB)\n",
            "Non-trainable params: 43101699 (164.42 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "227/227 [==============================] - 149s 596ms/step - loss: 0.4097 - accuracy: 0.8117 - val_loss: 0.3641 - val_accuracy: 0.8319\n",
            "Epoch 2/3\n",
            "227/227 [==============================] - 136s 600ms/step - loss: 0.2283 - accuracy: 0.9266 - val_loss: 0.2284 - val_accuracy: 0.9071\n",
            "Epoch 3/3\n",
            "227/227 [==============================] - 136s 600ms/step - loss: 0.1191 - accuracy: 0.9729 - val_loss: 0.1813 - val_accuracy: 0.9115\n",
            "29/29 [==============================] - 6s 220ms/step - loss: 0.1913 - accuracy: 0.9427\n",
            "Rank: 8, Test Loss: 0.19132132828235626, Test Accuracy: 0.9427312612533569\n",
            "Training with LoRA rank: 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_739', 'pre_classifier', 'classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_distil_bert_for_sequence_classification_36\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " distilbert (TFDistilBertMa  multiple                  67100160  \n",
            " inLayer)                                                        \n",
            "                                                                 \n",
            " pre_classifier (Dense)      multiple                  590592    \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  2307      \n",
            "                                                                 \n",
            " dropout_739 (Dropout)       multiple                  0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 67693059 (258.23 MB)\n",
            "Trainable params: 24591360 (93.81 MB)\n",
            "Non-trainable params: 43101699 (164.42 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "227/227 [==============================] - 152s 614ms/step - loss: 0.3815 - accuracy: 0.8332 - val_loss: 0.3053 - val_accuracy: 0.8628\n",
            "Epoch 2/3\n",
            "227/227 [==============================] - 133s 587ms/step - loss: 0.1830 - accuracy: 0.9475 - val_loss: 0.1892 - val_accuracy: 0.9292\n",
            "Epoch 3/3\n",
            "227/227 [==============================] - 137s 602ms/step - loss: 0.0904 - accuracy: 0.9779 - val_loss: 0.1484 - val_accuracy: 0.9381\n",
            "29/29 [==============================] - 6s 219ms/step - loss: 0.1646 - accuracy: 0.9427\n",
            "Rank: 16, Test Loss: 0.1646149456501007, Test Accuracy: 0.9427312612533569\n",
            "Training with LoRA rank: 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_759', 'pre_classifier', 'classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_distil_bert_for_sequence_classification_37\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " distilbert (TFDistilBertMa  multiple                  67837440  \n",
            " inLayer)                                                        \n",
            "                                                                 \n",
            " pre_classifier (Dense)      multiple                  590592    \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  2307      \n",
            "                                                                 \n",
            " dropout_759 (Dropout)       multiple                  0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 68430339 (261.04 MB)\n",
            "Trainable params: 25328640 (96.62 MB)\n",
            "Non-trainable params: 43101699 (164.42 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "227/227 [==============================] - 150s 599ms/step - loss: 0.3473 - accuracy: 0.8647 - val_loss: 0.2311 - val_accuracy: 0.9027\n",
            "Epoch 2/3\n",
            "227/227 [==============================] - 136s 601ms/step - loss: 0.1320 - accuracy: 0.9608 - val_loss: 0.1694 - val_accuracy: 0.9381\n",
            "Epoch 3/3\n",
            "227/227 [==============================] - 137s 602ms/step - loss: 0.0609 - accuracy: 0.9845 - val_loss: 0.1500 - val_accuracy: 0.9381\n",
            "29/29 [==============================] - 6s 220ms/step - loss: 0.1506 - accuracy: 0.9559\n",
            "Rank: 32, Test Loss: 0.15063133835792542, Test Accuracy: 0.9559471607208252\n",
            "Training with LoRA rank: 64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'pre_classifier', 'dropout_779']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_distil_bert_for_sequence_classification_38\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " distilbert (TFDistilBertMa  multiple                  69312000  \n",
            " inLayer)                                                        \n",
            "                                                                 \n",
            " pre_classifier (Dense)      multiple                  590592    \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  2307      \n",
            "                                                                 \n",
            " dropout_779 (Dropout)       multiple                  0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 69904899 (266.67 MB)\n",
            "Trainable params: 26803200 (102.25 MB)\n",
            "Non-trainable params: 43101699 (164.42 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "227/227 [==============================] - 149s 598ms/step - loss: 0.3133 - accuracy: 0.8741 - val_loss: 0.2282 - val_accuracy: 0.9204\n",
            "Epoch 2/3\n",
            "227/227 [==============================] - 137s 603ms/step - loss: 0.0978 - accuracy: 0.9740 - val_loss: 0.1352 - val_accuracy: 0.9469\n",
            "Epoch 3/3\n",
            "227/227 [==============================] - 133s 586ms/step - loss: 0.0445 - accuracy: 0.9917 - val_loss: 0.1109 - val_accuracy: 0.9602\n",
            "29/29 [==============================] - 6s 219ms/step - loss: 0.1438 - accuracy: 0.9515\n",
            "Rank: 64, Test Loss: 0.14382952451705933, Test Accuracy: 0.9515418410301208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result:\n",
        "\n",
        "\n",
        "| Rank | Total Parameters | Trainable Parameters | Non-trainable Parameters | Test Loss | Test Accuracy |\n",
        "|------|------------------|----------------------|--------------------------|-----------|---------------|\n",
        "| 2    | 67,047,939       | 23,946,240           | 43,101,699               | 0.2493    | 91.63%        |\n",
        "| 4    | 67,140,099       | 24,038,400           | 43,101,699               | 0.2167    | 92.95%        |\n",
        "| 8    | 67,324,419       | 24,222,720           | 43,101,699               | 0.1913    | 94.27%        |\n",
        "| 16   | 67,693,059       | 24,591,360           | 43,101,699               | 0.1646    | 94.27%        |\n",
        "| 32   | 68,430,339       | 25,328,640           | 43,101,699               | 0.1506    | 95.59%        |\n",
        "| 64   | 69,904,899       | 26,803,200           | 43,101,699               | 0.1438    | 95.15%        |\n"
      ],
      "metadata": {
        "id": "L2gPU9SaX8bV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(ranks, test_losses, marker='o', color='b')\n",
        "plt.title('Rank vs Test Loss')\n",
        "plt.xlabel('Rank')\n",
        "plt.ylabel('Test Loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(ranks, test_accuracies, marker='o', color='r')\n",
        "plt.title('Rank vs Test Accuracy')\n",
        "plt.xlabel('Rank')\n",
        "plt.ylabel('Test Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "Ik4eG29KBdgE",
        "outputId": "16643ae6-b91a-45e3-c6f8-5745a44dec26"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the rank as increase we can see an increase in accuracy but up to a point and after that it decreases as it might be overfitting.\n",
        "\n",
        "Increasing the rank is also increasing the computational resource required to train the model\n"
      ],
      "metadata": {
        "id": "nuYX9ykgO0Zg"
      }
    }
  ]
}